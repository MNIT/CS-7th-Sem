# -*- coding: utf-8 -*-
"""linear regression.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1JCc1ak06w1KfyQr4BvUI3AvVfrZQCMP8
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import matplotlib.animation as animation

df = pd.read_csv('data.txt', sep=',', header=None).values

#print(df.values)

x = np.array(df[:, 0])
y = np.array(df[:, 1])

m = len(y)

x.shape = (m, 1)
y.shape = (m, 1)

#print(m)

plt.plot(x, y, 'ro')
plt.xlabel('Profit')
plt.ylabel('Population of City')
plt.title('Linear Regression')
plt.show()

X = np.concatenate((np.ones([m, 1]), x), axis=1)
#print(X1)


#print(theta)

iterations = 1500
alpha = 0.01

"""**Compute Cost**"""

def compute_cost(theta):
    predictions = np.dot(X, theta)
    squareError = np.square(predictions - y)
    #print(squareError)
    J = np.sum(squareError) / (2 * m)
    return J

J = compute_cost(np.array([[-1], [2]]))
print(J)

"""**Minimize the loss**"""

def gradient_descent(w, alpha, iters):
    #w = np.ndarray.copy(w)
    J_history = np.zeros([iters, 1])
    
    for iter in range(iters):
        gradient = np.dot(X, w) - y
        
        for j in range(np.shape(X)[1]):
            w[j] = w[j] - (alpha / m) * np.sum(gradient * X[:, j:j+1])
                
        #print(gradient)
        J_history[iter] = compute_cost(w)
        #print(iter, J_history[iter])
    
    #print(w)
    return w, J_history

theta = np.zeros([np.shape(X)[1], 1])
w1, J_history_1 = gradient_descent(np.zeros([np.shape(X)[1], 1]), 0.01, 1500)
w2, J_history_2 = gradient_descent(np.zeros([np.shape(X)[1], 1]), 0.001, 1500)
w3, J_history_3 = gradient_descent(np.zeros([np.shape(X)[1], 1]), 0.03, 1500)
w4, J_history_4 = gradient_descent(np.zeros([np.shape(X)[1], 1]), 0.003, 1500)
print(w1, w2, w3, w4)

def plot_graph(w):
    plt.plot(x, y, 'ro')
    plt.xlabel('Profit')
    plt.ylabel('Population of City')
    plt.title('Linear Regression')
    plt.plot(X[:, 1:2], np.dot(X, w))
    plt.show()

plot_graph(w1)
plot_graph(w2)
plot_graph(w3)
plot_graph(w4)

"""**Decreasing cost with number of iterations**"""

def plot_cost_and_itrs(J_history, alpha):
    itrs = [x for x in range(1, iterations+1)]
    plt.plot(itrs, J_history)
    plt.xlabel('Number of iterations')
    plt.ylabel('J')
    plt.title("Alpha = " + alpha)
    plt.show()

plot_cost_and_itrs(J_history_1, "0.01")
plot_cost_and_itrs(J_history_2, "0.001")
plot_cost_and_itrs(J_history_3, "0.03")
plot_cost_and_itrs(J_history_4, "0.003")

"""**Normal Equation**"""

theta = np.dot(np.dot(np.linalg.inv(np.dot(X.T, X)), X.T), y)
print(theta)
plot_graph(theta)

